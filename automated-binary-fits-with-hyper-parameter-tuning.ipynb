{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score,roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\nfrom tensorflow import keras\n# from keras.losses import SparseCategoricalCrossentropy\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import callbacks,layers\n\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:24.310371Z","iopub.execute_input":"2023-09-13T19:07:24.310891Z","iopub.status.idle":"2023-09-13T19:07:35.513730Z","shell.execute_reply.started":"2023-09-13T19:07:24.310845Z","shell.execute_reply":"2023-09-13T19:07:35.512534Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Before the ML modeling, a preprocessing must be done in the dataset. We will use Normalization, Outlier Filtering, Correlation Filtering and Equalization.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ids-intrusion-csv/02-14-2018.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:35.516296Z","iopub.execute_input":"2023-09-13T19:07:35.517108Z","iopub.status.idle":"2023-09-13T19:07:50.535796Z","shell.execute_reply.started":"2023-09-13T19:07:35.517072Z","shell.execute_reply":"2023-09-13T19:07:50.534519Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Preliminary Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"Check existence and drop -inf and +inf values","metadata":{}},{"cell_type":"code","source":"# replace +ve and -ve infinity with NaN\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\n# Drop all NaN values\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:50.537358Z","iopub.execute_input":"2023-09-13T19:07:50.537687Z","iopub.status.idle":"2023-09-13T19:07:52.655536Z","shell.execute_reply.started":"2023-09-13T19:07:50.537658Z","shell.execute_reply":"2023-09-13T19:07:52.654340Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"count = df['Label'].value_counts()  # Perform value count on the 'A' column\nprint(f\"Value counts for dataframe:\\n{count}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:52.657169Z","iopub.execute_input":"2023-09-13T19:07:52.657802Z","iopub.status.idle":"2023-09-13T19:07:52.851565Z","shell.execute_reply.started":"2023-09-13T19:07:52.657759Z","shell.execute_reply":"2023-09-13T19:07:52.850419Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Value counts for dataframe:\nBenign            663808\nFTP-BruteForce    193354\nSSH-Bruteforce    187589\nName: Label, dtype: int64\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Convert Timestamp values to pandas date and time datetime64 format","metadata":{}},{"cell_type":"code","source":"df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:52.856112Z","iopub.execute_input":"2023-09-13T19:07:52.856560Z","iopub.status.idle":"2023-09-13T19:07:53.420696Z","shell.execute_reply.started":"2023-09-13T19:07:52.856518Z","shell.execute_reply":"2023-09-13T19:07:53.419530Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Convert datetime64 format to epoch format (epoch is Unix epoch time format, wich represents the number of seconds elapsed since January 1, 1970, at 00:00:00 UTC)","metadata":{}},{"cell_type":"code","source":"df['Timestamp'] = (df['Timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:53.422448Z","iopub.execute_input":"2023-09-13T19:07:53.422772Z","iopub.status.idle":"2023-09-13T19:07:53.446900Z","shell.execute_reply.started":"2023-09-13T19:07:53.422743Z","shell.execute_reply":"2023-09-13T19:07:53.445665Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Change all misclassified datatypes to float and substitute errors by NaN","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    #Check if the datatype of the column is object\n    if df[col].dtype == 'object' and col != 'Label':\n        # Change all values to numeric, and to NaN if it is a strig\n        df[col] = pd.to_numeric(df[col], errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:53.448475Z","iopub.execute_input":"2023-09-13T19:07:53.452171Z","iopub.status.idle":"2023-09-13T19:07:53.461647Z","shell.execute_reply.started":"2023-09-13T19:07:53.452126Z","shell.execute_reply":"2023-09-13T19:07:53.460388Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Count and drop NaN values","metadata":{}},{"cell_type":"code","source":"print(df.isna().sum())\ndf.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:53.462943Z","iopub.execute_input":"2023-09-13T19:07:53.463316Z","iopub.status.idle":"2023-09-13T19:07:54.879225Z","shell.execute_reply.started":"2023-09-13T19:07:53.463283Z","shell.execute_reply":"2023-09-13T19:07:54.878181Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Dst Port         0\nProtocol         0\nTimestamp        0\nFlow Duration    0\nTot Fwd Pkts     0\n                ..\nIdle Mean        0\nIdle Std         0\nIdle Max         0\nIdle Min         0\nLabel            0\nLength: 80, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Outliers Filtering","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\n# Define a function to filter outliers using Z-score\ndef filter_outliers_zscore(data, threshold):\n    z_scores = np.abs(stats.zscore(data))\n    outlier_mask = (z_scores > threshold).any(axis=1)\n    return data[~outlier_mask], data[outlier_mask]\n\n# Define a threshold value\nthreshold = 7\n\n\n# # The filtering was removing all DDoS-LOIC-UDP, so we will not execute it on them\n\n# if key == '02-21-2018':\n#     df_temp = df[df['Label'] == 'DDOS attack-LOIC-UDP']\n#     df = df[df['Label'] != 'DDOS attack-LOIC-UDP']\n\n\n\n\n# Loop through the columns of the dataframe and filter outliers in each column\nfiltered_cols = []\nremoved_outliers = []\nfor col in df.columns:\n    if col != 'Label':\n        filtered_col, outliers = filter_outliers_zscore(df[[col]], threshold)\n\n        filtered_cols.append(filtered_col)\n        removed_outliers.append(outliers)\n\n# Combine the filtered columns back into a dataframe\ndf_filtered = pd.concat(filtered_cols, axis=1)\n\n# Combine the removed outliers back into a dataframe\ndf_outliers = pd.concat(removed_outliers, axis=1)\n\n\n\n\n\n\n# Dataframe filtering comparison\n\nprint(f'\\nDataframe Shape: {df.shape}')\n\n# Print the number of outliers removed for each column\n\nprint('Outlier removal summary:')\nn_outliers = df_outliers.shape[0]\nprint(f'{n_outliers} outliers rows to be removed')\n\n# Print the original dataframe and the filtered dataframe side by side\nprint('\\nOriginal dataframe:')\ndisplay(df.head())\n\n# Assign filtered dataframe columns to original one\ncolumns = [col for col in df.columns if col != 'Label']\ndf.loc[:,columns] = df_filtered.loc[:,columns]\n\n\n# # Recombine rows from 'DDOS attack-LOIC-UDP'\n# if key == '02-21-2018':\n#     df = pd.concat([df,df_temp])\n\n\nprint('\\nFiltered dataframe:')\ndisplay(df.head())\n\n# Print the removed outliers dataframe\nprint('\\nRemoved outliers:')\ndisplay(df_outliers.head())\n\n# Count how many rows of each attack were removed from original dataframe\nvalues_orig = df.loc[df.index.isin(df_outliers.index), 'Label']\nprint(f'\\n{values_orig.value_counts()}')\n\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(inplace=True)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:07:54.880860Z","iopub.execute_input":"2023-09-13T19:07:54.881333Z","iopub.status.idle":"2023-09-13T19:08:15.706711Z","shell.execute_reply.started":"2023-09-13T19:07:54.881291Z","shell.execute_reply":"2023-09-13T19:08:15.705516Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nDataframe Shape: (1044751, 80)\nOutlier removal summary:\n33654 outliers rows to be removed\n\nOriginal dataframe:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Dst Port  Protocol   Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n0         0         0  1518597061      112641719             3             0   \n1         0         0  1518597230      112641466             3             0   \n2         0         0  1518597399      112638623             3             0   \n3        22         6  1518597613        6453966            15            10   \n4        22         6  1518597623        8804066            14            11   \n\n   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  ...  \\\n0                0                0                0                0  ...   \n1                0                0                0                0  ...   \n2                0                0                0                0  ...   \n3             1239             2273              744                0  ...   \n4             1143             2209              744                0  ...   \n\n   Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  \\\n0                 0          0.0         0.0           0           0   \n1                 0          0.0         0.0           0           0   \n2                 0          0.0         0.0           0           0   \n3                32          0.0         0.0           0           0   \n4                32          0.0         0.0           0           0   \n\n    Idle Mean    Idle Std  Idle Max  Idle Min   Label  \n0  56320859.5  139.300036  56320958  56320761  Benign  \n1  56320733.0  114.551299  56320814  56320652  Benign  \n2  56319311.5  301.934596  56319525  56319098  Benign  \n3         0.0    0.000000         0         0  Benign  \n4         0.0    0.000000         0         0  Benign  \n\n[5 rows x 80 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1518597061</td>\n      <td>112641719</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>56320859.5</td>\n      <td>139.300036</td>\n      <td>56320958</td>\n      <td>56320761</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1518597230</td>\n      <td>112641466</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>56320733.0</td>\n      <td>114.551299</td>\n      <td>56320814</td>\n      <td>56320652</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1518597399</td>\n      <td>112638623</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>56319311.5</td>\n      <td>301.934596</td>\n      <td>56319525</td>\n      <td>56319098</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1518597613</td>\n      <td>6453966</td>\n      <td>15</td>\n      <td>10</td>\n      <td>1239</td>\n      <td>2273</td>\n      <td>744</td>\n      <td>0</td>\n      <td>...</td>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1518597623</td>\n      <td>8804066</td>\n      <td>14</td>\n      <td>11</td>\n      <td>1143</td>\n      <td>2209</td>\n      <td>744</td>\n      <td>0</td>\n      <td>...</td>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nFiltered dataframe:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Dst Port  Protocol     Timestamp  Flow Duration  Tot Fwd Pkts  \\\n0         0         0  1.518597e+09    112641719.0           3.0   \n1         0         0  1.518597e+09    112641466.0           3.0   \n2         0         0  1.518597e+09    112638623.0           3.0   \n3        22         6  1.518598e+09      6453966.0          15.0   \n4        22         6  1.518598e+09      8804066.0          14.0   \n\n   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n0           0.0              0.0              0.0              0.0   \n1           0.0              0.0              0.0              0.0   \n2           0.0              0.0              0.0              0.0   \n3          10.0           1239.0           2273.0            744.0   \n4          11.0           1143.0           2209.0            744.0   \n\n   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n0              0.0  ...                 0          0.0         0.0   \n1              0.0  ...                 0          0.0         0.0   \n2              0.0  ...                 0          0.0         0.0   \n3              0.0  ...                32          0.0         0.0   \n4              0.0  ...                32          0.0         0.0   \n\n   Active Max  Active Min   Idle Mean    Idle Std    Idle Max    Idle Min  \\\n0         0.0         0.0  56320859.5  139.300036  56320958.0  56320761.0   \n1         0.0         0.0  56320733.0  114.551299  56320814.0  56320652.0   \n2         0.0         0.0  56319311.5  301.934596  56319525.0  56319098.0   \n3         0.0         0.0         0.0    0.000000         0.0         0.0   \n4         0.0         0.0         0.0    0.000000         0.0         0.0   \n\n    Label  \n0  Benign  \n1  Benign  \n2  Benign  \n3  Benign  \n4  Benign  \n\n[5 rows x 80 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.518597e+09</td>\n      <td>112641719.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>56320859.5</td>\n      <td>139.300036</td>\n      <td>56320958.0</td>\n      <td>56320761.0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.518597e+09</td>\n      <td>112641466.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>56320733.0</td>\n      <td>114.551299</td>\n      <td>56320814.0</td>\n      <td>56320652.0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1.518597e+09</td>\n      <td>112638623.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>56319311.5</td>\n      <td>301.934596</td>\n      <td>56319525.0</td>\n      <td>56319098.0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1.518598e+09</td>\n      <td>6453966.0</td>\n      <td>15.0</td>\n      <td>10.0</td>\n      <td>1239.0</td>\n      <td>2273.0</td>\n      <td>744.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22</td>\n      <td>6</td>\n      <td>1.518598e+09</td>\n      <td>8804066.0</td>\n      <td>14.0</td>\n      <td>11.0</td>\n      <td>1143.0</td>\n      <td>2209.0</td>\n      <td>744.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>32</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nRemoved outliers:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        Dst Port  Protocol  Timestamp  Flow Duration  Tot Fwd Pkts  \\\n410956       NaN       NaN   356477.0  -1.187300e+10           NaN   \n410957       NaN       NaN   631953.0  -6.814020e+11           NaN   \n410958       NaN       NaN   976676.0  -9.190110e+11           NaN   \n410959       NaN       NaN   983710.0  -2.738500e+11           NaN   \n412184       NaN       NaN   985452.0  -5.298010e+11         505.0   \n\n        Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n410956           NaN              NaN              NaN              NaN   \n410957           NaN              NaN              NaN              NaN   \n410958           NaN              NaN              NaN              NaN   \n410959           NaN              NaN              NaN              NaN   \n412184           NaN              NaN              NaN              NaN   \n\n        Fwd Pkt Len Min  ...  Fwd Act Data Pkts  Fwd Seg Size Min  \\\n410956              NaN  ...                NaN               NaN   \n410957              NaN  ...                NaN               NaN   \n410958              NaN  ...                NaN               NaN   \n410959              NaN  ...                NaN               NaN   \n412184              NaN  ...                NaN               NaN   \n\n        Active Mean  Active Std  Active Max  Active Min     Idle Mean  \\\n410956          NaN         NaN         NaN         NaN           NaN   \n410957          NaN         NaN         NaN         NaN           NaN   \n410958          NaN         NaN         NaN         NaN  2.841125e+11   \n410959          NaN         NaN         NaN         NaN  3.394503e+11   \n412184          NaN         NaN         NaN         NaN  3.337556e+11   \n\n            Idle Std      Idle Max      Idle Min  \n410956           NaN           NaN           NaN  \n410957           NaN           NaN           NaN  \n410958  1.931524e+11  7.548470e+11  1.260300e+10  \n410959  2.432682e+11  9.797810e+11  7.758000e+09  \n412184  2.377517e+11  9.484310e+11  4.908000e+09  \n\n[5 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Act Data Pkts</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>410956</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>356477.0</td>\n      <td>-1.187300e+10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>410957</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>631953.0</td>\n      <td>-6.814020e+11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>410958</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>976676.0</td>\n      <td>-9.190110e+11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.841125e+11</td>\n      <td>1.931524e+11</td>\n      <td>7.548470e+11</td>\n      <td>1.260300e+10</td>\n    </tr>\n    <tr>\n      <th>410959</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>983710.0</td>\n      <td>-2.738500e+11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.394503e+11</td>\n      <td>2.432682e+11</td>\n      <td>9.797810e+11</td>\n      <td>7.758000e+09</td>\n    </tr>\n    <tr>\n      <th>412184</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>985452.0</td>\n      <td>-5.298010e+11</td>\n      <td>505.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.337556e+11</td>\n      <td>2.377517e+11</td>\n      <td>9.484310e+11</td>\n      <td>4.908000e+09</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 79 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nBenign    33654\nName: Label, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"display(df.describe())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:15.708305Z","iopub.execute_input":"2023-09-13T19:08:15.708700Z","iopub.status.idle":"2023-09-13T19:08:18.489070Z","shell.execute_reply.started":"2023-09-13T19:08:15.708667Z","shell.execute_reply":"2023-09-13T19:08:18.487916Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"           Dst Port      Protocol     Timestamp  Flow Duration  Tot Fwd Pkts  \\\ncount  1.011097e+06  1.011097e+06  1.011097e+06   1.011097e+06  1.011097e+06   \nmean   4.617869e+03  8.169613e+00  1.518595e+09   6.366756e+06  5.005059e+00   \nstd    1.407674e+04  4.505407e+00  1.443707e+04   2.224621e+07  6.989601e+00   \nmin    0.000000e+00  0.000000e+00  1.518570e+09   1.000000e+00  1.000000e+00   \n25%    2.200000e+01  6.000000e+00  1.518578e+09   7.000000e+00  1.000000e+00   \n50%    5.300000e+01  6.000000e+00  1.518602e+09   9.510000e+02  2.000000e+00   \n75%    4.430000e+02  6.000000e+00  1.518607e+09   3.876120e+05  6.000000e+00   \nmax    6.553300e+04  1.700000e+01  1.518613e+09   1.200000e+08  2.650000e+02   \n\n       Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\ncount  1.011097e+06     1.011097e+06     1.011097e+06     1.011097e+06   \nmean   4.447752e+00     3.792241e+02     8.347496e+02     1.692143e+02   \nstd    7.286993e+00     7.507051e+02     3.461690e+03     2.685747e+02   \nmin    0.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n25%    1.000000e+00     0.000000e+00     0.000000e+00     0.000000e+00   \n50%    1.000000e+00     3.500000e+01     5.200000e+01     3.300000e+01   \n75%    5.000000e+00     3.640000e+02     5.540000e+02     1.610000e+02   \nmax    4.600000e+02     2.769000e+04     5.432450e+05     1.460000e+03   \n\n       Fwd Pkt Len Min  ...  Fwd Act Data Pkts  Fwd Seg Size Min  \\\ncount     1.011097e+06  ...       1.011097e+06      1.011097e+06   \nmean      8.315534e+00  ...       2.648352e+00      2.342916e+01   \nstd       1.700098e+01  ...       5.011792e+00      1.121421e+01   \nmin       0.000000e+00  ...       0.000000e+00      0.000000e+00   \n25%       0.000000e+00  ...       0.000000e+00      2.000000e+01   \n50%       0.000000e+00  ...       0.000000e+00      2.000000e+01   \n75%       0.000000e+00  ...       3.000000e+00      3.200000e+01   \nmax       1.370000e+02  ...       4.000000e+01      4.800000e+01   \n\n        Active Mean    Active Std    Active Max    Active Min     Idle Mean  \\\ncount  1.011097e+06  1.011097e+06  1.011097e+06  1.011097e+06  1.011097e+06   \nmean   1.470823e+04  7.105414e+03  3.260628e+04  1.184884e+04  1.143227e+06   \nstd    1.553650e+05  6.870820e+04  2.602018e+05  1.506151e+05  6.867112e+06   \nmin    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \nmax    4.067542e+06  1.554120e+06  5.243341e+06  3.959858e+06  1.199907e+08   \n\n           Idle Std      Idle Max      Idle Min  \ncount  1.011097e+06  1.011097e+06  1.011097e+06  \nmean   1.719559e+04  1.153016e+06  1.111306e+06  \nstd    3.440432e+05  6.894340e+06  6.829427e+06  \nmin    0.000000e+00  0.000000e+00  0.000000e+00  \n25%    0.000000e+00  0.000000e+00  0.000000e+00  \n50%    0.000000e+00  0.000000e+00  0.000000e+00  \n75%    0.000000e+00  0.000000e+00  0.000000e+00  \nmax    7.021683e+07  1.199907e+08  1.199907e+08  \n\n[8 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Act Data Pkts</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>...</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n      <td>1.011097e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.617869e+03</td>\n      <td>8.169613e+00</td>\n      <td>1.518595e+09</td>\n      <td>6.366756e+06</td>\n      <td>5.005059e+00</td>\n      <td>4.447752e+00</td>\n      <td>3.792241e+02</td>\n      <td>8.347496e+02</td>\n      <td>1.692143e+02</td>\n      <td>8.315534e+00</td>\n      <td>...</td>\n      <td>2.648352e+00</td>\n      <td>2.342916e+01</td>\n      <td>1.470823e+04</td>\n      <td>7.105414e+03</td>\n      <td>3.260628e+04</td>\n      <td>1.184884e+04</td>\n      <td>1.143227e+06</td>\n      <td>1.719559e+04</td>\n      <td>1.153016e+06</td>\n      <td>1.111306e+06</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.407674e+04</td>\n      <td>4.505407e+00</td>\n      <td>1.443707e+04</td>\n      <td>2.224621e+07</td>\n      <td>6.989601e+00</td>\n      <td>7.286993e+00</td>\n      <td>7.507051e+02</td>\n      <td>3.461690e+03</td>\n      <td>2.685747e+02</td>\n      <td>1.700098e+01</td>\n      <td>...</td>\n      <td>5.011792e+00</td>\n      <td>1.121421e+01</td>\n      <td>1.553650e+05</td>\n      <td>6.870820e+04</td>\n      <td>2.602018e+05</td>\n      <td>1.506151e+05</td>\n      <td>6.867112e+06</td>\n      <td>3.440432e+05</td>\n      <td>6.894340e+06</td>\n      <td>6.829427e+06</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>1.518570e+09</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.200000e+01</td>\n      <td>6.000000e+00</td>\n      <td>1.518578e+09</td>\n      <td>7.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>2.000000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.300000e+01</td>\n      <td>6.000000e+00</td>\n      <td>1.518602e+09</td>\n      <td>9.510000e+02</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>3.500000e+01</td>\n      <td>5.200000e+01</td>\n      <td>3.300000e+01</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>2.000000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.430000e+02</td>\n      <td>6.000000e+00</td>\n      <td>1.518607e+09</td>\n      <td>3.876120e+05</td>\n      <td>6.000000e+00</td>\n      <td>5.000000e+00</td>\n      <td>3.640000e+02</td>\n      <td>5.540000e+02</td>\n      <td>1.610000e+02</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>3.000000e+00</td>\n      <td>3.200000e+01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>6.553300e+04</td>\n      <td>1.700000e+01</td>\n      <td>1.518613e+09</td>\n      <td>1.200000e+08</td>\n      <td>2.650000e+02</td>\n      <td>4.600000e+02</td>\n      <td>2.769000e+04</td>\n      <td>5.432450e+05</td>\n      <td>1.460000e+03</td>\n      <td>1.370000e+02</td>\n      <td>...</td>\n      <td>4.000000e+01</td>\n      <td>4.800000e+01</td>\n      <td>4.067542e+06</td>\n      <td>1.554120e+06</td>\n      <td>5.243341e+06</td>\n      <td>3.959858e+06</td>\n      <td>1.199907e+08</td>\n      <td>7.021683e+07</td>\n      <td>1.199907e+08</td>\n      <td>1.199907e+08</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 79 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ncolumns = [col for col in df.columns if col != 'Label']\nmin_max_scaler = MinMaxScaler().fit(df[columns])\ndf[columns] = min_max_scaler.transform(df[columns])\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:18.490509Z","iopub.execute_input":"2023-09-13T19:08:18.490937Z","iopub.status.idle":"2023-09-13T19:08:26.198651Z","shell.execute_reply.started":"2023-09-13T19:08:18.490897Z","shell.execute_reply":"2023-09-13T19:08:26.197324Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Dst Port  Protocol  Timestamp  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n0  0.000000  0.000000   0.626427       0.938681      0.007576      0.000000   \n1  0.000000  0.000000   0.630339       0.938679      0.007576      0.000000   \n2  0.000000  0.000000   0.634251       0.938655      0.007576      0.000000   \n3  0.000336  0.352941   0.639205       0.053783      0.053030      0.021739   \n4  0.000336  0.352941   0.639436       0.073367      0.049242      0.023913   \n\n   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  ...  \\\n0         0.000000         0.000000         0.000000              0.0  ...   \n1         0.000000         0.000000         0.000000              0.0  ...   \n2         0.000000         0.000000         0.000000              0.0  ...   \n3         0.044745         0.004184         0.509589              0.0  ...   \n4         0.041278         0.004066         0.509589              0.0  ...   \n\n   Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  \\\n0          0.000000          0.0         0.0         0.0         0.0   \n1          0.000000          0.0         0.0         0.0         0.0   \n2          0.000000          0.0         0.0         0.0         0.0   \n3          0.666667          0.0         0.0         0.0         0.0   \n4          0.666667          0.0         0.0         0.0         0.0   \n\n   Idle Mean  Idle Std  Idle Max  Idle Min   Label  \n0   0.469377  0.000002  0.469378  0.469376  Benign  \n1   0.469376  0.000002  0.469376  0.469375  Benign  \n2   0.469364  0.000004  0.469366  0.469362  Benign  \n3   0.000000  0.000000  0.000000  0.000000  Benign  \n4   0.000000  0.000000  0.000000  0.000000  Benign  \n\n[5 rows x 80 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Idle Max</th>\n      <th>Idle Min</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.626427</td>\n      <td>0.938681</td>\n      <td>0.007576</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.469377</td>\n      <td>0.000002</td>\n      <td>0.469378</td>\n      <td>0.469376</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.630339</td>\n      <td>0.938679</td>\n      <td>0.007576</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.469376</td>\n      <td>0.000002</td>\n      <td>0.469376</td>\n      <td>0.469375</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.634251</td>\n      <td>0.938655</td>\n      <td>0.007576</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.469364</td>\n      <td>0.000004</td>\n      <td>0.469366</td>\n      <td>0.469362</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000336</td>\n      <td>0.352941</td>\n      <td>0.639205</td>\n      <td>0.053783</td>\n      <td>0.053030</td>\n      <td>0.021739</td>\n      <td>0.044745</td>\n      <td>0.004184</td>\n      <td>0.509589</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>Benign</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000336</td>\n      <td>0.352941</td>\n      <td>0.639436</td>\n      <td>0.073367</td>\n      <td>0.049242</td>\n      <td>0.023913</td>\n      <td>0.041278</td>\n      <td>0.004066</td>\n      <td>0.509589</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>Benign</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 80 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Feature Correlation Filtering","metadata":{}},{"cell_type":"code","source":"columns = [col for col in df.columns if col != 'Label']\n\ncorr_matrix = df[columns].corr().abs()\n\nthreshold = 0.99\n# Find features with high correlation\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\n# Print features to drop\nprint(f\"The following {len(to_drop)} features will be dropped due to high correlation: {to_drop}\")\n\ndf = df.drop(to_drop, axis = 1)\n\n# replace +ve and -ve infinity with NaN\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\n# Drop all NaN values\ndf.dropna(inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:26.200427Z","iopub.execute_input":"2023-09-13T19:08:26.200785Z","iopub.status.idle":"2023-09-13T19:08:46.147537Z","shell.execute_reply.started":"2023-09-13T19:08:26.200752Z","shell.execute_reply":"2023-09-13T19:08:46.146374Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"The following 16 features will be dropped due to high correlation: ['Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Min', 'Pkt Len Min', 'Pkt Len Max', 'SYN Flag Cnt', 'ECE Flag Cnt', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Idle Max', 'Idle Min']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Binarize the data for modeling","metadata":{}},{"cell_type":"code","source":"df[\"Label\"] = df.Label.map(lambda a:\"normal\" if a == 'Benign' else \"abnormal\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:46.149339Z","iopub.execute_input":"2023-09-13T19:08:46.149783Z","iopub.status.idle":"2023-09-13T19:08:46.475204Z","shell.execute_reply.started":"2023-09-13T19:08:46.149742Z","shell.execute_reply":"2023-09-13T19:08:46.473937Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Equalization","metadata":{}},{"cell_type":"code","source":"df = df.sample(frac=1) #Randomize rows's sequence\n\ndf2 = df[df[\"Label\"] == \"abnormal\"]\n\ndf0 = df[df[\"Label\"] == \"normal\"][:df2.shape[0]]\n\ndf = pd.concat([ df0,df2], axis =0)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:46.482958Z","iopub.execute_input":"2023-09-13T19:08:46.483339Z","iopub.status.idle":"2023-09-13T19:08:48.532227Z","shell.execute_reply.started":"2023-09-13T19:08:46.483308Z","shell.execute_reply":"2023-09-13T19:08:48.530970Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Final Preprocessed Data Visualization","metadata":{}},{"cell_type":"code","source":"print(f\"{df['Label'].value_counts()}\\n\")\ndisplay(df.describe())\ndisplay(df.head())\ndisplay(df.info())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:48.534158Z","iopub.execute_input":"2023-09-13T19:08:48.534543Z","iopub.status.idle":"2023-09-13T19:08:51.006175Z","shell.execute_reply.started":"2023-09-13T19:08:48.534509Z","shell.execute_reply":"2023-09-13T19:08:51.004808Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"normal      380943\nabnormal    380943\nName: Label, dtype: int64\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            Dst Port       Protocol      Timestamp  Flow Duration  \\\ncount  761886.000000  761886.000000  761886.000000   7.618860e+05   \nmean        0.056496       0.455563       0.562558   4.251198e-02   \nstd         0.194220       0.242906       0.340339   1.668585e-01   \nmin         0.000000       0.000000       0.000000   0.000000e+00   \n25%         0.000320       0.352941       0.169147   4.166667e-08   \n50%         0.000336       0.352941       0.731406   3.158334e-06   \n75%         0.001221       0.352941       0.859603   3.108846e-03   \nmax         1.000000       1.000000       1.000000   1.000000e+00   \n\n        Tot Fwd Pkts   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  \\\ncount  761886.000000  761886.000000    761886.000000    761886.000000   \nmean        0.016112       0.010328         0.014375         0.001466   \nstd         0.028402       0.016534         0.027588         0.005705   \nmin         0.000000       0.000000         0.000000         0.000000   \n25%         0.000000       0.002174         0.000000         0.000000   \n50%         0.000000       0.002174         0.000000         0.000000   \n75%         0.018939       0.010870         0.013146         0.000941   \nmax         1.000000       0.823913         0.960130         0.821256   \n\n       Fwd Pkt Len Max  Fwd Pkt Len Min  ...  Init Fwd Win Byts  \\\ncount    761886.000000    761886.000000  ...      761886.000000   \nmean          0.114369         0.048823  ...           0.192138   \nstd           0.184922         0.113947  ...           0.203880   \nmin           0.000000         0.000000  ...           0.000000   \n25%           0.000000         0.000000  ...           0.003693   \n50%           0.000000         0.000000  ...           0.125015   \n75%           0.110274         0.000000  ...           0.410217   \nmax           1.000000         1.000000  ...           1.000000   \n\n       Init Bwd Win Byts  Fwd Act Data Pkts  Fwd Seg Size Min    Active Mean  \\\ncount      761886.000000      761886.000000     761886.000000  761886.000000   \nmean            0.077633           0.072550          0.540122       0.002894   \nstd             0.254271           0.136435          0.236988       0.034203   \nmin             0.000000           0.000000          0.000000       0.000000   \n25%             0.000000           0.000000          0.416667       0.000000   \n50%             0.000015           0.000000          0.666667       0.000000   \n75%             0.003525           0.075000          0.833333       0.000000   \nmax             1.000000           1.000000          1.000000       1.000000   \n\n          Active Std     Active Max     Active Min      Idle Mean  \\\ncount  761886.000000  761886.000000  761886.000000  761886.000000   \nmean        0.003678       0.004983       0.002391       0.007606   \nstd         0.039721       0.044511       0.034039       0.051275   \nmin         0.000000       0.000000       0.000000       0.000000   \n25%         0.000000       0.000000       0.000000       0.000000   \n50%         0.000000       0.000000       0.000000       0.000000   \n75%         0.000000       0.000000       0.000000       0.000000   \nmax         1.000000       0.996383       0.998352       1.000000   \n\n            Idle Std  \ncount  761886.000000  \nmean        0.000193  \nstd         0.004410  \nmin         0.000000  \n25%         0.000000  \n50%         0.000000  \n75%         0.000000  \nmax         1.000000  \n\n[8 rows x 63 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Init Fwd Win Byts</th>\n      <th>Init Bwd Win Byts</th>\n      <th>Fwd Act Data Pkts</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>7.618860e+05</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>...</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n      <td>761886.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.056496</td>\n      <td>0.455563</td>\n      <td>0.562558</td>\n      <td>4.251198e-02</td>\n      <td>0.016112</td>\n      <td>0.010328</td>\n      <td>0.014375</td>\n      <td>0.001466</td>\n      <td>0.114369</td>\n      <td>0.048823</td>\n      <td>...</td>\n      <td>0.192138</td>\n      <td>0.077633</td>\n      <td>0.072550</td>\n      <td>0.540122</td>\n      <td>0.002894</td>\n      <td>0.003678</td>\n      <td>0.004983</td>\n      <td>0.002391</td>\n      <td>0.007606</td>\n      <td>0.000193</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.194220</td>\n      <td>0.242906</td>\n      <td>0.340339</td>\n      <td>1.668585e-01</td>\n      <td>0.028402</td>\n      <td>0.016534</td>\n      <td>0.027588</td>\n      <td>0.005705</td>\n      <td>0.184922</td>\n      <td>0.113947</td>\n      <td>...</td>\n      <td>0.203880</td>\n      <td>0.254271</td>\n      <td>0.136435</td>\n      <td>0.236988</td>\n      <td>0.034203</td>\n      <td>0.039721</td>\n      <td>0.044511</td>\n      <td>0.034039</td>\n      <td>0.051275</td>\n      <td>0.004410</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000320</td>\n      <td>0.352941</td>\n      <td>0.169147</td>\n      <td>4.166667e-08</td>\n      <td>0.000000</td>\n      <td>0.002174</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.003693</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.416667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000336</td>\n      <td>0.352941</td>\n      <td>0.731406</td>\n      <td>3.158334e-06</td>\n      <td>0.000000</td>\n      <td>0.002174</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.125015</td>\n      <td>0.000015</td>\n      <td>0.000000</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.001221</td>\n      <td>0.352941</td>\n      <td>0.859603</td>\n      <td>3.108846e-03</td>\n      <td>0.018939</td>\n      <td>0.010870</td>\n      <td>0.013146</td>\n      <td>0.000941</td>\n      <td>0.110274</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.410217</td>\n      <td>0.003525</td>\n      <td>0.075000</td>\n      <td>0.833333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000e+00</td>\n      <td>1.000000</td>\n      <td>0.823913</td>\n      <td>0.960130</td>\n      <td>0.821256</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.996383</td>\n      <td>0.998352</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 63 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        Dst Port  Protocol  Timestamp  Flow Duration  Tot Fwd Pkts  \\\n466902  0.006760  0.352941   0.883817       0.002450      0.030303   \n629969  0.006760  0.352941   0.925369       0.923539      0.060606   \n477636  0.000809  1.000000   0.088058       0.000211      0.003788   \n511872  0.000809  1.000000   0.990532       0.001568      0.003788   \n839069  0.000809  1.000000   0.287298       0.000003      0.000000   \n\n        Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n466902      0.015217         0.059010         0.008828         0.553425   \n629969      0.043478         0.020910         0.001668         0.354110   \n477636      0.004348         0.003106         0.000379         0.029452   \n511872      0.004348         0.002384         0.000394         0.022603   \n839069      0.002174         0.002600         0.000191         0.049315   \n\n        Fwd Pkt Len Min  ...  Init Bwd Win Byts  Fwd Act Data Pkts  \\\n466902         0.000000  ...           0.015594              0.100   \n629969         0.000000  ...           0.020020              0.325   \n477636         0.313869  ...           0.000000              0.025   \n511872         0.240876  ...           0.000000              0.025   \n839069         0.525547  ...           0.000000              0.000   \n\n        Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  \\\n466902          0.416667     0.000000    0.000000    0.000000    0.000000   \n629969          0.416667     0.018092    0.080194    0.077484    0.005269   \n477636          0.166667     0.000000    0.000000    0.000000    0.000000   \n511872          0.166667     0.000000    0.000000    0.000000    0.000000   \n839069          0.166667     0.000000    0.000000    0.000000    0.000000   \n\n        Idle Mean  Idle Std   Label  \n466902   0.000000  0.000000  normal  \n629969   0.083335  0.000329  normal  \n477636   0.000000  0.000000  normal  \n511872   0.000000  0.000000  normal  \n839069   0.000000  0.000000  normal  \n\n[5 rows x 64 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dst Port</th>\n      <th>Protocol</th>\n      <th>Timestamp</th>\n      <th>Flow Duration</th>\n      <th>Tot Fwd Pkts</th>\n      <th>Tot Bwd Pkts</th>\n      <th>TotLen Fwd Pkts</th>\n      <th>TotLen Bwd Pkts</th>\n      <th>Fwd Pkt Len Max</th>\n      <th>Fwd Pkt Len Min</th>\n      <th>...</th>\n      <th>Init Bwd Win Byts</th>\n      <th>Fwd Act Data Pkts</th>\n      <th>Fwd Seg Size Min</th>\n      <th>Active Mean</th>\n      <th>Active Std</th>\n      <th>Active Max</th>\n      <th>Active Min</th>\n      <th>Idle Mean</th>\n      <th>Idle Std</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>466902</th>\n      <td>0.006760</td>\n      <td>0.352941</td>\n      <td>0.883817</td>\n      <td>0.002450</td>\n      <td>0.030303</td>\n      <td>0.015217</td>\n      <td>0.059010</td>\n      <td>0.008828</td>\n      <td>0.553425</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.015594</td>\n      <td>0.100</td>\n      <td>0.416667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>629969</th>\n      <td>0.006760</td>\n      <td>0.352941</td>\n      <td>0.925369</td>\n      <td>0.923539</td>\n      <td>0.060606</td>\n      <td>0.043478</td>\n      <td>0.020910</td>\n      <td>0.001668</td>\n      <td>0.354110</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.020020</td>\n      <td>0.325</td>\n      <td>0.416667</td>\n      <td>0.018092</td>\n      <td>0.080194</td>\n      <td>0.077484</td>\n      <td>0.005269</td>\n      <td>0.083335</td>\n      <td>0.000329</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>477636</th>\n      <td>0.000809</td>\n      <td>1.000000</td>\n      <td>0.088058</td>\n      <td>0.000211</td>\n      <td>0.003788</td>\n      <td>0.004348</td>\n      <td>0.003106</td>\n      <td>0.000379</td>\n      <td>0.029452</td>\n      <td>0.313869</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.025</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>511872</th>\n      <td>0.000809</td>\n      <td>1.000000</td>\n      <td>0.990532</td>\n      <td>0.001568</td>\n      <td>0.003788</td>\n      <td>0.004348</td>\n      <td>0.002384</td>\n      <td>0.000394</td>\n      <td>0.022603</td>\n      <td>0.240876</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.025</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>839069</th>\n      <td>0.000809</td>\n      <td>1.000000</td>\n      <td>0.287298</td>\n      <td>0.000003</td>\n      <td>0.000000</td>\n      <td>0.002174</td>\n      <td>0.002600</td>\n      <td>0.000191</td>\n      <td>0.049315</td>\n      <td>0.525547</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 761886 entries, 466902 to 34171\nData columns (total 64 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   Dst Port           761886 non-null  float64\n 1   Protocol           761886 non-null  float64\n 2   Timestamp          761886 non-null  float64\n 3   Flow Duration      761886 non-null  float64\n 4   Tot Fwd Pkts       761886 non-null  float64\n 5   Tot Bwd Pkts       761886 non-null  float64\n 6   TotLen Fwd Pkts    761886 non-null  float64\n 7   TotLen Bwd Pkts    761886 non-null  float64\n 8   Fwd Pkt Len Max    761886 non-null  float64\n 9   Fwd Pkt Len Min    761886 non-null  float64\n 10  Fwd Pkt Len Mean   761886 non-null  float64\n 11  Fwd Pkt Len Std    761886 non-null  float64\n 12  Bwd Pkt Len Max    761886 non-null  float64\n 13  Bwd Pkt Len Min    761886 non-null  float64\n 14  Bwd Pkt Len Mean   761886 non-null  float64\n 15  Bwd Pkt Len Std    761886 non-null  float64\n 16  Flow Byts/s        761886 non-null  float64\n 17  Flow Pkts/s        761886 non-null  float64\n 18  Flow IAT Mean      761886 non-null  float64\n 19  Flow IAT Std       761886 non-null  float64\n 20  Flow IAT Max       761886 non-null  float64\n 21  Flow IAT Min       761886 non-null  float64\n 22  Fwd IAT Std        761886 non-null  float64\n 23  Fwd IAT Max        761886 non-null  float64\n 24  Bwd IAT Tot        761886 non-null  float64\n 25  Bwd IAT Mean       761886 non-null  float64\n 26  Bwd IAT Std        761886 non-null  float64\n 27  Bwd IAT Max        761886 non-null  float64\n 28  Bwd IAT Min        761886 non-null  float64\n 29  Fwd PSH Flags      761886 non-null  float64\n 30  Bwd PSH Flags      761886 non-null  float64\n 31  Fwd URG Flags      761886 non-null  float64\n 32  Bwd URG Flags      761886 non-null  float64\n 33  Fwd Header Len     761886 non-null  float64\n 34  Bwd Header Len     761886 non-null  float64\n 35  Fwd Pkts/s         761886 non-null  float64\n 36  Bwd Pkts/s         761886 non-null  float64\n 37  Pkt Len Mean       761886 non-null  float64\n 38  Pkt Len Std        761886 non-null  float64\n 39  Pkt Len Var        761886 non-null  float64\n 40  FIN Flag Cnt       761886 non-null  float64\n 41  RST Flag Cnt       761886 non-null  float64\n 42  PSH Flag Cnt       761886 non-null  float64\n 43  ACK Flag Cnt       761886 non-null  float64\n 44  URG Flag Cnt       761886 non-null  float64\n 45  CWE Flag Count     761886 non-null  float64\n 46  Down/Up Ratio      761886 non-null  float64\n 47  Fwd Byts/b Avg     761886 non-null  float64\n 48  Fwd Pkts/b Avg     761886 non-null  float64\n 49  Fwd Blk Rate Avg   761886 non-null  float64\n 50  Bwd Byts/b Avg     761886 non-null  float64\n 51  Bwd Pkts/b Avg     761886 non-null  float64\n 52  Bwd Blk Rate Avg   761886 non-null  float64\n 53  Init Fwd Win Byts  761886 non-null  float64\n 54  Init Bwd Win Byts  761886 non-null  float64\n 55  Fwd Act Data Pkts  761886 non-null  float64\n 56  Fwd Seg Size Min   761886 non-null  float64\n 57  Active Mean        761886 non-null  float64\n 58  Active Std         761886 non-null  float64\n 59  Active Max         761886 non-null  float64\n 60  Active Min         761886 non-null  float64\n 61  Idle Mean          761886 non-null  float64\n 62  Idle Std           761886 non-null  float64\n 63  Label              761886 non-null  object \ndtypes: float64(63), object(1)\nmemory usage: 377.8+ MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"markdown","source":"The dataframe is using more memory than the original one because there are more decimal precision and less null values. So, even if as we droped a lot of values, the preprocessed dataframe uses more memory. To contour this, we will round the data. Keep in mind that this will cause some loss of precision.","metadata":{}},{"cell_type":"code","source":"# Round the numeric columns to the specified decimal places\ndf_rounded = df.round(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:51.007529Z","iopub.execute_input":"2023-09-13T19:08:51.007879Z","iopub.status.idle":"2023-09-13T19:08:51.379908Z","shell.execute_reply.started":"2023-09-13T19:08:51.007846Z","shell.execute_reply":"2023-09-13T19:08:51.378503Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"display(df_rounded.info())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:51.381506Z","iopub.execute_input":"2023-09-13T19:08:51.381923Z","iopub.status.idle":"2023-09-13T19:08:51.729778Z","shell.execute_reply.started":"2023-09-13T19:08:51.381889Z","shell.execute_reply":"2023-09-13T19:08:51.728436Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 761886 entries, 466902 to 34171\nData columns (total 64 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   Dst Port           761886 non-null  float64\n 1   Protocol           761886 non-null  float64\n 2   Timestamp          761886 non-null  float64\n 3   Flow Duration      761886 non-null  float64\n 4   Tot Fwd Pkts       761886 non-null  float64\n 5   Tot Bwd Pkts       761886 non-null  float64\n 6   TotLen Fwd Pkts    761886 non-null  float64\n 7   TotLen Bwd Pkts    761886 non-null  float64\n 8   Fwd Pkt Len Max    761886 non-null  float64\n 9   Fwd Pkt Len Min    761886 non-null  float64\n 10  Fwd Pkt Len Mean   761886 non-null  float64\n 11  Fwd Pkt Len Std    761886 non-null  float64\n 12  Bwd Pkt Len Max    761886 non-null  float64\n 13  Bwd Pkt Len Min    761886 non-null  float64\n 14  Bwd Pkt Len Mean   761886 non-null  float64\n 15  Bwd Pkt Len Std    761886 non-null  float64\n 16  Flow Byts/s        761886 non-null  float64\n 17  Flow Pkts/s        761886 non-null  float64\n 18  Flow IAT Mean      761886 non-null  float64\n 19  Flow IAT Std       761886 non-null  float64\n 20  Flow IAT Max       761886 non-null  float64\n 21  Flow IAT Min       761886 non-null  float64\n 22  Fwd IAT Std        761886 non-null  float64\n 23  Fwd IAT Max        761886 non-null  float64\n 24  Bwd IAT Tot        761886 non-null  float64\n 25  Bwd IAT Mean       761886 non-null  float64\n 26  Bwd IAT Std        761886 non-null  float64\n 27  Bwd IAT Max        761886 non-null  float64\n 28  Bwd IAT Min        761886 non-null  float64\n 29  Fwd PSH Flags      761886 non-null  float64\n 30  Bwd PSH Flags      761886 non-null  float64\n 31  Fwd URG Flags      761886 non-null  float64\n 32  Bwd URG Flags      761886 non-null  float64\n 33  Fwd Header Len     761886 non-null  float64\n 34  Bwd Header Len     761886 non-null  float64\n 35  Fwd Pkts/s         761886 non-null  float64\n 36  Bwd Pkts/s         761886 non-null  float64\n 37  Pkt Len Mean       761886 non-null  float64\n 38  Pkt Len Std        761886 non-null  float64\n 39  Pkt Len Var        761886 non-null  float64\n 40  FIN Flag Cnt       761886 non-null  float64\n 41  RST Flag Cnt       761886 non-null  float64\n 42  PSH Flag Cnt       761886 non-null  float64\n 43  ACK Flag Cnt       761886 non-null  float64\n 44  URG Flag Cnt       761886 non-null  float64\n 45  CWE Flag Count     761886 non-null  float64\n 46  Down/Up Ratio      761886 non-null  float64\n 47  Fwd Byts/b Avg     761886 non-null  float64\n 48  Fwd Pkts/b Avg     761886 non-null  float64\n 49  Fwd Blk Rate Avg   761886 non-null  float64\n 50  Bwd Byts/b Avg     761886 non-null  float64\n 51  Bwd Pkts/b Avg     761886 non-null  float64\n 52  Bwd Blk Rate Avg   761886 non-null  float64\n 53  Init Fwd Win Byts  761886 non-null  float64\n 54  Init Bwd Win Byts  761886 non-null  float64\n 55  Fwd Act Data Pkts  761886 non-null  float64\n 56  Fwd Seg Size Min   761886 non-null  float64\n 57  Active Mean        761886 non-null  float64\n 58  Active Std         761886 non-null  float64\n 59  Active Max         761886 non-null  float64\n 60  Active Min         761886 non-null  float64\n 61  Idle Mean          761886 non-null  float64\n 62  Idle Std           761886 non-null  float64\n 63  Label              761886 non-null  object \ndtypes: float64(63), object(1)\nmemory usage: 377.8+ MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}}]},{"cell_type":"markdown","source":"It still displays the same memory usage, but when we save it to a .csv file it will be significantly smaller (in this run it went from 400+ MB to 210 MB)","metadata":{}},{"cell_type":"markdown","source":"## Save preprocessed dataset","metadata":{}},{"cell_type":"code","source":"df_rounded.to_csv('/kaggle/working/preproc_data.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:08:51.731835Z","iopub.execute_input":"2023-09-13T19:08:51.732321Z","iopub.status.idle":"2023-09-13T19:09:37.263027Z","shell.execute_reply.started":"2023-09-13T19:08:51.732277Z","shell.execute_reply":"2023-09-13T19:09:37.261839Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Building functions for modeling","metadata":{}},{"cell_type":"code","source":"def splits_assemble(path, test_size=0.3):\n    # Import dataset\n    df_dataset = pd.read_csv(path)\n\n    # Encode Labels for numeric classification\n    label_encoder = LabelEncoder()\n    df_dataset['Label'] = label_encoder.fit_transform(df_dataset['Label'])\n\n    # Set a random state for sampling\n    RANDOM_STATE_SEED = np.random.randint(123)\n    print(f'Random Seed:{RANDOM_STATE_SEED}')\n    \n    # Split dataset in train and test\n    train, test = train_test_split(df_dataset, test_size=test_size, random_state=RANDOM_STATE_SEED)\n    \n    # Count how many instances there are in each label\n    print(df_dataset[\"Label\"].value_counts())\n\n    # Separate in X and y for better classification\n    y_train = np.array(train.pop(\"Label\"))# pop removes \"Label\" from the dataframe\n    X_train = train.values\n\n    print(f'Tipo X_train: {type(X_train)} Tipo y_train: {type(y_train)} Shape X_train:{X_train.shape} Shape y_train: {y_train.shape}')\n\n    y_test = np.array(test.pop(\"Label\")) # pop removes \"Label\" from the dataframe\n    X_test = test.values\n\n    print(f'Tipo X_test: {type(X_test)} Tipo y_test: {type(y_test)} Shape X_test:{X_test.shape} Shape y_test: {y_test.shape}')\n    \n    return X_train,y_train,X_test,y_test","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:09:37.264602Z","iopub.execute_input":"2023-09-13T19:09:37.264962Z","iopub.status.idle":"2023-09-13T19:09:37.276066Z","shell.execute_reply.started":"2023-09-13T19:09:37.264930Z","shell.execute_reply":"2023-09-13T19:09:37.275060Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def models_assemble(X_train):\n    models = {}\n\n    models['DT'] = DecisionTreeClassifier()\n    models['RF'] = RandomForestClassifier()\n    models['SVM'] = LinearSVC(max_iter=10000, dual=False,)\n    models['KNN'] = KNeighborsClassifier(algorithm='ball_tree')\n    models['NB'] = GaussianNB()\n    models['XGB'] = xgb.XGBClassifier()\n    models['NN'] = keras.Sequential([\n            layers.InputLayer(input_shape=(X_train.shape[1],)),\n            \n            layers.BatchNormalization(renorm=True),\n            layers.Dense(128, activation='relu'),\n            layers.Dropout(rate = 0.3),\n            layers.BatchNormalization(renorm=True),\n            layers.Dense(64, activation='relu'),\n            layers.Dropout(rate = 0.3),\n            layers.BatchNormalization(renorm=True),\n            layers.Dense(32, activation='relu'),\n            layers.Dropout(rate = 0.3),\n            layers.Dense(1, activation='sigmoid'),\n        ])\n    return models","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:09:37.277668Z","iopub.execute_input":"2023-09-13T19:09:37.278050Z","iopub.status.idle":"2023-09-13T19:09:37.295338Z","shell.execute_reply.started":"2023-09-13T19:09:37.278017Z","shell.execute_reply":"2023-09-13T19:09:37.293854Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def grids_assemble(cv=2, model=None,key=None):\n    \n    hyperparameters = {}\n    hyperparameters['XGB'] = {'learning_rate': [0.9, 0.7, 0.5, 0.3, 0.1], 'n_estimators': [50,100,150,200],\n                              }\n    hyperparameters['DT'] = {'criterion': ['gini','entropy'], 'max_depth': [10,15,20,25,30],'splitter':['best','random']\n                             }\n    hyperparameters['RF'] = {'n_estimators': [50, 75, 100, 125, 150], 'criterion': ['gini','entropy'],'max_depth': [25,30]\n                             }\n    hyperparameters['SVM'] = { 'C': np.linspace(0.01,100, num=20)\n                              }\n    hyperparameters['NB'] = {'var_smoothing': np.logspace(0,-9, num=20)\n                             }\n    hyperparameters['KNN'] = {'n_neighbors': [8,9,10,11,12], 'weights': ['uniform','distance'], 'leaf_size': [10,100]\n                            }\n\n    classifierGRID = GridSearchCV(\n        estimator = model,\n        param_grid = hyperparameters[key],\n        cv=cv,\n        verbose=1,\n        n_jobs=-1  # Use all Available CPU cores # If \"-x\", use all -x available CPU cores\n    )\n    return classifierGRID","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:09:37.296820Z","iopub.execute_input":"2023-09-13T19:09:37.297233Z","iopub.status.idle":"2023-09-13T19:09:37.314159Z","shell.execute_reply.started":"2023-09-13T19:09:37.297193Z","shell.execute_reply":"2023-09-13T19:09:37.313285Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def fit_assemble(classifierGRID,X_train,y_train):\n    classifierGRID.fit(X=X_train, y=y_train)\n\n    # Print best parameters found on GridsearchCV\n    print(\"Accuracy score on Validation set: \\n\")\n    print(classifierGRID.best_score_ )\n    print(\"---------------\")\n    print(\"Best performing hyperparameters on Validation set: \")\n    print(classifierGRID.best_estimator_)\n    print(\"---------------\")\n    \n\n\n    fitted_model = classifierGRID.best_estimator_\n    \n    return fitted_model","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:09:37.315872Z","iopub.execute_input":"2023-09-13T19:09:37.316194Z","iopub.status.idle":"2023-09-13T19:09:37.334261Z","shell.execute_reply.started":"2023-09-13T19:09:37.316167Z","shell.execute_reply":"2023-09-13T19:09:37.333229Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def NeuralNet_fit(neuralNetModel,X_train,y_train,cv=2):\n    # %%\n    def neuralnet_create(epsilon):\n        model=keras.Sequential([\n                layers.InputLayer(input_shape=(X_train.shape[1],)),\n                \n                layers.BatchNormalization(renorm=True),\n                layers.Dense(128, activation='relu'),\n                layers.Dropout(rate = 0.3),\n                layers.BatchNormalization(renorm=True),\n                layers.Dense(64, activation='relu'),\n                layers.Dropout(rate = 0.3),\n                layers.BatchNormalization(renorm=True),\n                layers.Dense(32, activation='relu'),\n                layers.Dropout(rate = 0.3),\n                layers.Dense(1, activation='sigmoid'),\n            ])\n        optimizer = tf.keras.optimizers.Adam(epsilon=epsilon)\n\n        model.compile(\n            optimizer=optimizer,\n            loss='binary_crossentropy',\n            metrics=['binary_accuracy'],\n        )\n        return model\n    # def neuralnet_create(epsilon):\n    #     model=keras.Sequential([\n    #             layers.InputLayer(input_shape=(X_train.shape[1],)),\n                \n    #             layers.BatchNormalization(renorm=True),\n    #             layers.Dense(128, activation='relu'),\n    #             layers.Dropout(rate = 0.3),\n    #             layers.BatchNormalization(renorm=True),\n    #             layers.Dense(64, activation='relu'),\n    #             layers.Dropout(rate = 0.3),\n    #             layers.BatchNormalization(renorm=True),\n    #             layers.Dense(32, activation='relu'),\n    #             layers.Dropout(rate = 0.3),\n    #             layers.Dense(2, activation='linear'),\n    #         ])\n    #     optimizer = tf.keras.optimizers.Adam(epsilon=epsilon)\n\n    #     model.compile(\n    #         optimizer=optimizer,\n    #         loss= SparseCategoricalCrossentropy(from_logits=True),\n    #         metrics=['binary_accuracy'],\n    #     )\n    #     return model\n    # def lstm_autoencoder_create(epsilon):\n    #     model=keras.Sequential([\n    #             layers.LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1)),\n    #             layers.Dropout(rate = 0.3),\n    #             layers.LSTM(units=50,return_sequences=True),\n    #             layers.Dropout(rate = 0.3),\n    #             layers.LSTM(units=50),\n    #             layers.Dropout(rate = 0.3),\n    #             layers.Dense(units=1),\n\n    #         ])\n    #     optimizer = tf.keras.optimizers.Adam(epsilon=epsilon)\n\n    #     model.compile(\n    #         optimizer=optimizer,\n    #         loss='binary_crossentropy',\n    #         metrics=['binary_accuracy'],\n    #     )\n    #     return model\n\n\n    early_stopping = callbacks.EarlyStopping(\n        min_delta = 0.001,\n        patience = 7,\n        restore_best_weights = True,\n        monitor= 'loss'\n    )\n\n    hyperparameters = {'epochs': [50], 'batch_size': [128,256], 'callbacks':[early_stopping], 'epsilon':[0.001]\n                            }\n\n\n\n    neuralNetModel = KerasClassifier(build_fn=neuralnet_create)\n\n    classifierGRID = GridSearchCV(\n        estimator = neuralNetModel,\n        param_grid = hyperparameters,\n        cv=cv,\n        verbose=1,\n        n_jobs=-1,  # Use all -4 available CPU cores\n        scoring='accuracy'\n    )\n\n    classifierGRID = classifierGRID.fit(X=X_train, y=y_train)\n\n    # Print best parameters found on GridsearchCV\n    print(\"Accuracy score on Validation set: \\n\")\n    print(classifierGRID.best_score_ )\n    print(\"---------------\")\n    print(\"Best performing hyperparameters on Validation set: \")\n    print(classifierGRID.best_params_)\n    print(\"---------------\")\n    \n    fitted_model = classifierGRID.best_estimator_\n    \n    return fitted_model\n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:09:37.336138Z","iopub.execute_input":"2023-09-13T19:09:37.336912Z","iopub.status.idle":"2023-09-13T19:09:37.352967Z","shell.execute_reply.started":"2023-09-13T19:09:37.336873Z","shell.execute_reply":"2023-09-13T19:09:37.352018Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def metrics_assemble(fitted_model,X_test,y_test):\n    \n    predictions = fitted_model.predict(X_test)\n\n    if predictions.dtype == 'float32':\n        predictions = (fitted_model.predict(X_test) > 0.5).astype(\"int32\")\n\n    \n    accuracy = accuracy_score(y_test,predictions)\n    precision = precision_score(y_test,predictions, average='weighted')\n    recall = recall_score(y_test,predictions,average='weighted')\n    f1= f1_score(y_test,predictions,average='weighted')\n    auc= roc_auc_score(y_test,predictions,average='weighted')\n\n    return accuracy, precision, recall, f1, auc","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:09:37.354553Z","iopub.execute_input":"2023-09-13T19:09:37.354905Z","iopub.status.idle":"2023-09-13T19:09:37.375629Z","shell.execute_reply.started":"2023-09-13T19:09:37.354874Z","shell.execute_reply":"2023-09-13T19:09:37.374109Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Run ASSEMBLE","metadata":{}},{"cell_type":"code","source":"# Define path to the dataset\npath = '/kaggle/working/preproc_data.csv'\n\n# Start dictionaries to store metrics\naccuracy, precision, recall, f1, auc, fit_time, eval_time, fitted_models = {}, {}, {}, {}, {}, {}, {}, {} \n\n# Get dataset splits for tranning and evaluation\nX_train,y_train,X_test,y_test = splits_assemble(path, test_size=0.99)\n\n# Start dictionary for models\nmodels=models_assemble(X_train)\n\n# Start iteration loop for fitting and evaluating the classic models\nfor key in ['DT', 'RF', 'XGB', 'NB']: #'KNN', 'SVM',\n    \n    # Get the grid of hyperparameters and start the GridsearchCV function\n    classifierGRID = grids_assemble(cv = 2, model=models[key],key=key)\n\n    print(f'Fitting {key} model')\n\n    # Fit model\n    start_time = time.time()\n    fitted_models[key] = fit_assemble(classifierGRID,X_train,y_train)\n    end_time = time.time()\n    fit_time[key] = end_time - start_time\n\n    # Evaluate model\n    start_time = time.time()\n    accuracy[key], precision[key], recall[key], f1[key], auc[key] = metrics_assemble(fitted_models[key],X_test,y_test)\n    end_time = time.time()\n    eval_time[key] = end_time - start_time\n\n# Fit the neural network model\nstart_time = time.time()\nfitted_models['NN'] = NeuralNet_fit(models['NN'],X_train,y_train, cv = 2)\nend_time = time.time()\nfit_time['NN'] = end_time - start_time\n\n# Evaluate the neural network model\nstart_time = time.time()\naccuracy['NN'], precision['NN'], recall['NN'], f1['NN'], auc['NN'] = metrics_assemble(fitted_models['NN'],X_test,y_test)\nend_time = time.time()\neval_time['NN'] = end_time - start_time\n\n# Convert the metrics dictionaries into a dataframe for better visualization\nmetrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-Score':f1, 'ROC-AUC-Score': auc, \\\n           'Tempo':fit_time, 'Evaluation Time':eval_time}\ndf_metrics = pd.DataFrame(metrics)\ndisplay(df_metrics)  \n","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:34:15.037413Z","iopub.execute_input":"2023-09-13T19:34:15.038019Z","iopub.status.idle":"2023-09-13T19:41:01.228619Z","shell.execute_reply.started":"2023-09-13T19:34:15.037955Z","shell.execute_reply":"2023-09-13T19:41:01.225380Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Random Seed:92\n1    380943\n0    380943\nName: Label, dtype: int64\nTipo X_train: <class 'numpy.ndarray'> Tipo y_train: <class 'numpy.ndarray'> Shape X_train:(7618, 63) Shape y_train: (7618,)\nTipo X_test: <class 'numpy.ndarray'> Tipo y_test: <class 'numpy.ndarray'> Shape X_test:(754268, 63) Shape y_test: (754268,)\nFitting DT model\nFitting 2 folds for each of 20 candidates, totalling 40 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Accuracy score on Validation set: \n\n0.9998687319506432\n---------------\nBest performing hyperparameters on Validation set: \nDecisionTreeClassifier(max_depth=10, splitter='random')\n---------------\nFitting RF model\nFitting 2 folds for each of 20 candidates, totalling 40 fits\nAccuracy score on Validation set: \n\n1.0\n---------------\nBest performing hyperparameters on Validation set: \nRandomForestClassifier(max_depth=25, n_estimators=75)\n---------------\nFitting XGB model\nFitting 2 folds for each of 20 candidates, totalling 40 fits\nAccuracy score on Validation set: \n\n0.9997374639012864\n---------------\nBest performing hyperparameters on Validation set: \nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.9, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=50, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...)\n---------------\nFitting KNN model\nFitting 2 folds for each of 20 candidates, totalling 40 fits\nAccuracy score on Validation set: \n\n0.9990811236545025\n---------------\nBest performing hyperparameters on Validation set: \nKNeighborsClassifier(algorithm='ball_tree', leaf_size=10, n_neighbors=8,\n                     weights='distance')\n---------------\nFitting SVM model\nFitting 2 folds for each of 20 candidates, totalling 40 fits\nAccuracy score on Validation set: \n\n0.9997374639012864\n---------------\nBest performing hyperparameters on Validation set: \nLinearSVC(C=10.535263157894736, dual=False, max_iter=10000)\n---------------\nFitting NB model\nFitting 2 folds for each of 20 candidates, totalling 40 fits\nAccuracy score on Validation set: \n\n0.9998687319506432\n---------------\nBest performing hyperparameters on Validation set: \nGaussianNB(var_smoothing=8.858667904100832e-09)\n---------------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/4116286988.py:82: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n  neuralNetModel = KerasClassifier(build_fn=neuralnet_create)\n","output_type":"stream"},{"name":"stdout","text":"Fitting 2 folds for each of 2 candidates, totalling 4 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\nEpoch 1/50\nEpoch 1/50\nEpoch 1/50\n15/15 [==============================] - 5s 20ms/step - loss: 0.6914 - binary_accuracy: 0.5177\nEpoch 2/50\n15/15 [==============================] - 5s 21ms/step - loss: 0.6936 - binary_accuracy: 0.5056\nEpoch 2/50\n30/30 [==============================] - 5s 13ms/step - loss: 0.6576 - binary_accuracy: 0.6209\nEpoch 2/50\n30/30 [==============================] - 5s 13ms/step - loss: 0.6542 - binary_accuracy: 0.5957\nEpoch 2/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.6459 - binary_accuracy: 0.6437\nEpoch 3/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.6613 - binary_accuracy: 0.6141\nEpoch 3/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.4762 - binary_accuracy: 0.8706\n 9/15 [=================>............] - ETA: 0s - loss: 0.6017 - binary_accuracy: 0.7214Epoch 3/50\n30/30 [==============================] - 0s 11ms/step - loss: 0.4751 - binary_accuracy: 0.8370\nEpoch 3/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.5528 - binary_accuracy: 0.7923\nEpoch 4/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.5765 - binary_accuracy: 0.7527\n 1/15 [=>............................] - ETA: 0s - loss: 0.4891 - binary_accuracy: 0.8672Epoch 4/50\n15/15 [==============================] - 0s 18ms/step - loss: 0.3986 - binary_accuracy: 0.9100\nEpoch 5/50\n30/30 [==============================] - 0s 11ms/step - loss: 0.1861 - binary_accuracy: 0.9693\nEpoch 4/50\n15/15 [==============================] - 0s 17ms/step - loss: 0.4411 - binary_accuracy: 0.8561\nEpoch 5/50\n30/30 [==============================] - 0s 12ms/step - loss: 0.1929 - binary_accuracy: 0.9688\n 1/30 [>.............................] - ETA: 0s - loss: 0.1204 - binary_accuracy: 0.9766Epoch 4/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.2239 - binary_accuracy: 0.9606\nEpoch 6/50\n15/15 [==============================] - 0s 17ms/step - loss: 0.2736 - binary_accuracy: 0.9472\nEpoch 6/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0545 - binary_accuracy: 0.9945\nEpoch 5/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0474 - binary_accuracy: 0.9942\n 1/30 [>.............................] - ETA: 0s - loss: 0.0195 - binary_accuracy: 1.0000Epoch 5/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.0995 - binary_accuracy: 0.9866\nEpoch 7/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.1462 - binary_accuracy: 0.9814\nEpoch 7/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0181 - binary_accuracy: 0.9982\nEpoch 6/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0221 - binary_accuracy: 0.9966\nEpoch 6/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.0469 - binary_accuracy: 0.9937\nEpoch 8/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0682 - binary_accuracy: 0.9934\n14/30 [=============>................] - ETA: 0s - loss: 0.0171 - binary_accuracy: 0.9972Epoch 8/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0084 - binary_accuracy: 0.9979\nEpoch 7/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0170 - binary_accuracy: 0.9966\n13/15 [=========================>....] - ETA: 0s - loss: 0.0331 - binary_accuracy: 0.9967Epoch 7/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0246 - binary_accuracy: 0.9966\nEpoch 9/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0339 - binary_accuracy: 0.9963\nEpoch 9/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0179 - binary_accuracy: 0.9974\nEpoch 10/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0196 - binary_accuracy: 0.9987\nEpoch 10/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0060 - binary_accuracy: 0.9995\nEpoch 8/50\n30/30 [==============================] - 0s 11ms/step - loss: 0.0106 - binary_accuracy: 0.9982\nEpoch 8/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0142 - binary_accuracy: 0.9976\nEpoch 11/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0133 - binary_accuracy: 0.9982\nEpoch 11/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0065 - binary_accuracy: 0.9992\nEpoch 9/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0101 - binary_accuracy: 0.9971\nEpoch 9/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0105 - binary_accuracy: 0.9987\nEpoch 12/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0105 - binary_accuracy: 0.9984\nEpoch 12/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0052 - binary_accuracy: 0.9995\nEpoch 10/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0085 - binary_accuracy: 0.9987\n 1/30 [>.............................] - ETA: 0s - loss: 0.0070 - binary_accuracy: 1.0000Epoch 13/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0095 - binary_accuracy: 0.9979\nEpoch 13/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0063 - binary_accuracy: 0.9984\nEpoch 10/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0107 - binary_accuracy: 0.9982\nEpoch 14/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0074 - binary_accuracy: 0.9989\nEpoch 14/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0063 - binary_accuracy: 0.9982\n26/30 [=========================>....] - ETA: 0s - loss: 0.0059 - binary_accuracy: 0.9991Epoch 11/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0063 - binary_accuracy: 0.9989\nEpoch 11/50\n15/15 [==============================] - 0s 17ms/step - loss: 0.0067 - binary_accuracy: 0.9987\nEpoch 15/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0082 - binary_accuracy: 0.9992\n17/30 [================>.............] - ETA: 0s - loss: 0.0033 - binary_accuracy: 1.0000Epoch 15/50\n30/30 [==============================] - 0s 11ms/step - loss: 0.0040 - binary_accuracy: 0.9997\nEpoch 12/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0058 - binary_accuracy: 0.9987\n15/15 [==============================] - 0s 14ms/step - loss: 0.0068 - binary_accuracy: 0.9989\nEpoch 16/50\nEpoch 16/50\n30/30 [==============================] - 0s 12ms/step - loss: 0.0059 - binary_accuracy: 0.9992\nEpoch 12/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0072 - binary_accuracy: 0.9989\nEpoch 17/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0059 - binary_accuracy: 0.9984\nEpoch 17/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0042 - binary_accuracy: 1.0000\nEpoch 13/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0051 - binary_accuracy: 0.9992\nEpoch 13/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0071 - binary_accuracy: 0.9979\n15/15 [==============================] - 0s 14ms/step - loss: 0.0050 - binary_accuracy: 0.9992\n18/30 [=================>............] - ETA: 0s - loss: 0.0050 - binary_accuracy: 0.9991Epoch 18/50\nEpoch 18/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0045 - binary_accuracy: 0.9995\nEpoch 14/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0062 - binary_accuracy: 0.9982\nEpoch 19/50\n30/30 [==============================] - 0s 16ms/step - loss: 0.0057 - binary_accuracy: 0.9989\nEpoch 14/50\n15/15 [==============================] - 0s 28ms/step - loss: 0.0077 - binary_accuracy: 0.9987\nEpoch 19/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.0051 - binary_accuracy: 0.9989\nEpoch 20/50\n30/30 [==============================] - 0s 11ms/step - loss: 0.0043 - binary_accuracy: 0.9992\nEpoch 15/50\n30/30 [==============================] - 0s 12ms/step - loss: 0.0036 - binary_accuracy: 1.0000\n23/30 [======================>.......] - ETA: 0s - loss: 0.0032 - binary_accuracy: 0.9997Epoch 15/50\n15/15 [==============================] - 0s 20ms/step - loss: 0.0043 - binary_accuracy: 0.9992\n29/30 [============================>.] - ETA: 0s - loss: 0.0047 - binary_accuracy: 0.9995Epoch 20/50\n15/15 [==============================] - 0s 22ms/step - loss: 0.0046 - binary_accuracy: 0.9995\nEpoch 21/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0046 - binary_accuracy: 0.9995\n 7/30 [======>.......................] - ETA: 0s - loss: 0.0070 - binary_accuracy: 0.9967Epoch 16/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.0057 - binary_accuracy: 0.9992\nEpoch 21/50\n15/15 [==============================] - 0s 18ms/step - loss: 0.0035 - binary_accuracy: 0.9997\nEpoch 22/50\n30/30 [==============================] - 0s 13ms/step - loss: 0.0060 - binary_accuracy: 0.9987\nEpoch 16/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0051 - binary_accuracy: 0.9997\nEpoch 22/50\n30/30 [==============================] - 1s 17ms/step - loss: 0.0028 - binary_accuracy: 0.9997\nEpoch 17/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.0055 - binary_accuracy: 0.9987\nEpoch 23/50\n15/15 [==============================] - 0s 17ms/step - loss: 0.0052 - binary_accuracy: 0.9989\nEpoch 23/50\n15/15 [==============================] - 0s 16ms/step - loss: 0.0041 - binary_accuracy: 0.9997\nEpoch 24/50\n30/30 [==============================] - 0s 15ms/step - loss: 0.0038 - binary_accuracy: 0.9995\nEpoch 17/50\n15/15 [==============================] - 0s 18ms/step - loss: 0.0040 - binary_accuracy: 0.9997\nEpoch 24/50\n30/30 [==============================] - 0s 15ms/step - loss: 0.0034 - binary_accuracy: 0.9995\nEpoch 18/50\n15/15 [==============================] - 0s 21ms/step - loss: 0.0035 - binary_accuracy: 1.0000\nEpoch 25/50\n30/30 [==============================] - 0s 12ms/step - loss: 0.0034 - binary_accuracy: 0.9995\nEpoch 18/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0044 - binary_accuracy: 0.9992\nEpoch 25/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0021 - binary_accuracy: 0.9997\nEpoch 19/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0034 - binary_accuracy: 0.9995\nEpoch 26/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0041 - binary_accuracy: 1.0000\n27/30 [==========================>...] - ETA: 0s - loss: 0.0031 - binary_accuracy: 0.9994Epoch 26/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0030 - binary_accuracy: 0.9995\nEpoch 19/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0033 - binary_accuracy: 0.9997\nEpoch 27/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0025 - binary_accuracy: 0.9995\nEpoch 20/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0044 - binary_accuracy: 0.9997\nEpoch 27/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0025 - binary_accuracy: 0.9997\nEpoch 20/50\n15/15 [==============================] - 0s 15ms/step - loss: 0.0041 - binary_accuracy: 0.9995\nEpoch 28/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0019 - binary_accuracy: 1.0000\nEpoch 21/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0039 - binary_accuracy: 0.9989\nEpoch 28/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0029 - binary_accuracy: 0.9997\n15/15 [==============================] - 0s 16ms/step - loss: 0.0036 - binary_accuracy: 0.9995\nEpoch 21/50\n15/15 [==============================] - 0s 13ms/step - loss: 0.0041 - binary_accuracy: 0.9995\nEpoch 29/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0016 - binary_accuracy: 1.0000\nEpoch 22/50\n15/15 [==============================] - 0s 14ms/step - loss: 0.0032 - binary_accuracy: 0.9997\nEpoch 30/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0019 - binary_accuracy: 0.9997\nEpoch 22/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0032 - binary_accuracy: 0.9992\nEpoch 23/50\n15/15 [==============================] - 0s 18ms/step - loss: 0.0032 - binary_accuracy: 0.9995\n30/30 [==============================] - 0s 10ms/step - loss: 0.0023 - binary_accuracy: 0.9995\nEpoch 23/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0029 - binary_accuracy: 0.9997\nEpoch 24/50\n30/30 [==============================] - 0s 10ms/step - loss: 0.0014 - binary_accuracy: 1.0000\nEpoch 24/50\n30/30 [==============================] - 0s 9ms/step - loss: 0.0011 - binary_accuracy: 1.0000\nEpoch 25/50\n120/120 [==============================] - 1s 4ms/step\n30/30 [==============================] - 0s 9ms/step - loss: 0.0047 - binary_accuracy: 0.9997\nEpoch 25/50\n30/30 [==============================] - 0s 8ms/step - loss: 0.0012 - binary_accuracy: 0.9997\nEpoch 26/50\n30/30 [==============================] - 0s 7ms/step - loss: 9.6655e-04 - binary_accuracy: 1.0000\nEpoch 26/50\n120/120 [==============================] - 1s 3ms/steps: 0.0014 - binary_accuracy: 0.991.000\n30/30 [==============================] - 0s 8ms/step - loss: 0.0014 - binary_accuracy: 0.9995\nEpoch 27/50\n30/30 [==============================] - 0s 8ms/step - loss: 0.0011 - binary_accuracy: 0.9997\nEpoch 27/50\n30/30 [==============================] - 0s 7ms/step - loss: 8.1263e-04 - binary_accuracy: 1.0000\nEpoch 28/50\n30/30 [==============================] - 0s 8ms/step - loss: 0.0012 - binary_accuracy: 1.0000\nEpoch 28/50\n30/30 [==============================] - 0s 10ms/step - loss: 7.1442e-04 - binary_accuracy: 1.0000\n30/30 [==============================] - 0s 8ms/step - loss: 0.0011 - binary_accuracy: 0.9997\nEpoch 29/50\n30/30 [==============================] - 0s 7ms/step - loss: 0.0012 - binary_accuracy: 0.9997\nEpoch 30/50\n30/30 [==============================] - 0s 10ms/step - loss: 6.4916e-04 - binary_accuracy: 1.0000\n120/120 [==============================] - 1s 3ms/step\n120/120 [==============================] - 0s 2ms/step\nEpoch 1/50\n60/60 [==============================] - 3s 7ms/step - loss: 0.5522 - binary_accuracy: 0.7348\nEpoch 2/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0924 - binary_accuracy: 0.9900\nEpoch 3/50\n60/60 [==============================] - 0s 6ms/step - loss: 0.0126 - binary_accuracy: 0.9980\nEpoch 4/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0073 - binary_accuracy: 0.9987\nEpoch 5/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0059 - binary_accuracy: 0.9988\nEpoch 6/50\n60/60 [==============================] - 0s 6ms/step - loss: 0.0066 - binary_accuracy: 0.9988\nEpoch 7/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0056 - binary_accuracy: 0.9991\nEpoch 8/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0040 - binary_accuracy: 0.9995\nEpoch 9/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0036 - binary_accuracy: 0.9996\nEpoch 10/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0022 - binary_accuracy: 0.9999\nEpoch 11/50\n60/60 [==============================] - 0s 6ms/step - loss: 0.0019 - binary_accuracy: 0.9997\nEpoch 12/50\n60/60 [==============================] - 0s 7ms/step - loss: 0.0011 - binary_accuracy: 1.0000\nEpoch 13/50\n60/60 [==============================] - 0s 6ms/step - loss: 0.0020 - binary_accuracy: 0.9996\nEpoch 14/50\n60/60 [==============================] - 0s 6ms/step - loss: 0.0013 - binary_accuracy: 0.9999\nEpoch 15/50\n60/60 [==============================] - 0s 7ms/step - loss: 8.7106e-04 - binary_accuracy: 1.0000\nEpoch 16/50\n60/60 [==============================] - 0s 7ms/step - loss: 8.8371e-04 - binary_accuracy: 0.9999\nEpoch 17/50\n60/60 [==============================] - 0s 7ms/step - loss: 6.7841e-04 - binary_accuracy: 1.0000\nEpoch 18/50\n60/60 [==============================] - 0s 7ms/step - loss: 7.3479e-04 - binary_accuracy: 0.9999\nEpoch 19/50\n60/60 [==============================] - 0s 7ms/step - loss: 4.8221e-04 - binary_accuracy: 1.0000\nAccuracy score on Validation set: \n\n0.9994749278025729\n---------------\nBest performing hyperparameters on Validation set: \n{'batch_size': 128, 'callbacks': <keras.callbacks.EarlyStopping object at 0x796899f5f940>, 'epochs': 50, 'epsilon': 0.001}\n---------------\n23571/23571 [==============================] - 52s 2ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-Score\u001b[39m\u001b[38;5;124m'\u001b[39m:f1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC-AUC-Score\u001b[39m\u001b[38;5;124m'\u001b[39m: auc, \\\n\u001b[1;32m     47\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTempo\u001b[39m\u001b[38;5;124m'\u001b[39m:fit_time, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluation Time\u001b[39m\u001b[38;5;124m'\u001b[39m:eval_time}\n\u001b[1;32m     48\u001b[0m df_metrics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(metrics)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_metrics\u001b[49m\u001b[43m)\u001b[49m  \n","\u001b[0;31mTypeError\u001b[0m: 'RocCurveDisplay' object is not callable"],"ename":"TypeError","evalue":"'RocCurveDisplay' object is not callable","output_type":"error"}]},{"cell_type":"markdown","source":"## Save Model's metrics for future analysis","metadata":{}},{"cell_type":"code","source":"df_metrics.to_csv('/kaggle/working/metrics.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:13:38.586477Z","iopub.status.idle":"2023-09-13T19:13:38.587786Z","shell.execute_reply.started":"2023-09-13T19:13:38.587457Z","shell.execute_reply":"2023-09-13T19:13:38.587493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick Confusion Matrix visualization","metadata":{}},{"cell_type":"code","source":"\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\npredictions = fitted_models['DT'].predict(X_test)\ncf_matrix = confusion_matrix(y_test, predictions)\nsns.heatmap(cf_matrix, annot=True)  ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:30:12.715187Z","iopub.execute_input":"2023-09-13T19:30:12.715770Z","iopub.status.idle":"2023-09-13T19:30:13.118526Z","shell.execute_reply.started":"2023-09-13T19:30:12.715630Z","shell.execute_reply":"2023-09-13T19:30:13.117206Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# from sklearn.metrics import roc_curve\n# clf = fitted_models['KNN']\n# y = y_test\n# pred = clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:48:09.852792Z","iopub.execute_input":"2023-09-13T19:48:09.853409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# roc_auc_score(y_test,pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:47:57.444467Z","iopub.execute_input":"2023-09-13T19:47:57.445057Z","iopub.status.idle":"2023-09-13T19:47:57.658314Z","shell.execute_reply.started":"2023-09-13T19:47:57.444985Z","shell.execute_reply":"2023-09-13T19:47:57.657089Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"0.9999496202750265"},"metadata":{}}]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import numpy as np\n# from sklearn import metrics\n\n# fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n# roc_auc = metrics.auc(fpr, tpr)\n# display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n#                                   estimator_name='example estimator')\n# display.plot()\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:46:08.239282Z","iopub.execute_input":"2023-09-13T19:46:08.240294Z","iopub.status.idle":"2023-09-13T19:47:03.143957Z","shell.execute_reply.started":"2023-09-13T19:46:08.240249Z","shell.execute_reply":"2023-09-13T19:47:03.142559Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"23571/23571 [==============================] - 45s 2ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# plt.style.use('_mpl-gallery')\n\n# # plot\n# fig, ax = plt.subplots()\n\n# ax.plot(fpr, tpr, linewidth=2.0)\n\n# ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n#        ylim=(0, 8), yticks=np.arange(1, 8))\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T19:21:07.113086Z","iopub.execute_input":"2023-09-13T19:21:07.113569Z","iopub.status.idle":"2023-09-13T19:21:07.438590Z","shell.execute_reply.started":"2023-09-13T19:21:07.113531Z","shell.execute_reply":"2023-09-13T19:21:07.437305Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 200x200 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}